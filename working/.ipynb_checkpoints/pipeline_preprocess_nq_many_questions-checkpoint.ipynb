{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import requests\n",
    "import time\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from transformers import BertConfig, BertTokenizer, RobertaConfig, RobertaTokenizer\n",
    "from transformers import TFBertMainLayer, TFBertPreTrainedModel, TFRobertaMainLayer, TFRobertaPreTrainedModel\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "from tensorflow.keras import layers as L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup model , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertForNaturalQuestionAnswering(TFBertPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.initializer = get_initializer(config.initializer_range)\n",
    "        self.qa_outputs = L.Dense(config.num_labels,\n",
    "            kernel_initializer=self.initializer, name='qa_outputs')\n",
    "        self.long_outputs = L.Dense(1, kernel_initializer=self.initializer,\n",
    "            name='long_outputs')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, -1)\n",
    "        end_logits = tf.squeeze(end_logits, -1)\n",
    "        long_logits = tf.squeeze(self.long_outputs(sequence_output), -1)\n",
    "        return start_logits, end_logits, long_logits\n",
    "\n",
    "\n",
    "class TFRobertaForNaturalQuestionAnswering(TFRobertaPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = TFRobertaMainLayer(config, name='roberta')\n",
    "        self.initializer = get_initializer(config.initializer_range)\n",
    "        self.qa_outputs = L.Dense(config.num_labels,\n",
    "            kernel_initializer=self.initializer, name='qa_outputs')\n",
    "        self.long_outputs = L.Dense(1, kernel_initializer=self.initializer,\n",
    "            name='long_outputs')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.roberta(inputs, **kwargs)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, -1)\n",
    "        end_logits = tf.squeeze(end_logits, -1)\n",
    "        long_logits = tf.squeeze(self.long_outputs(sequence_output), -1)\n",
    "        return start_logits, end_logits, long_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, TFBertForNaturalQuestionAnswering, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, TFRobertaForNaturalQuestionAnswering, RobertaTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = \"../input/transformers_cache/bert_large_uncased_config.json\"\n",
    "do_lower_case = True\n",
    "model_type = 'bert'\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "config = config_class.from_json_file(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_add_tokens(do_enumerate):\n",
    "    tags = ['Dd', 'Dl', 'Dt', 'H1', 'H2', 'H3', 'Li', 'Ol', 'P', 'Table', 'Td', 'Th', 'Tr', 'Ul']\n",
    "    opening_tags = ['<{}>'.format(tag) for tag in tags]\n",
    "    closing_tags = ['</{}>'.format(tag) for tag in tags]\n",
    "    added_tags = opening_tags + closing_tags\n",
    "    # See `nq_to_sqaud.py` for special-tokens\n",
    "    special_tokens = ['<P>', '<Table>']\n",
    "    if do_enumerate:\n",
    "        for special_token in special_tokens:\n",
    "            for j in range(11):\n",
    "              added_tags.append('<{}{}>'.format(special_token[1: -1],j))\n",
    "\n",
    "    add_tokens = ['Td_colspan', 'Th_colspan', '``', '\\'\\'', '--']\n",
    "    add_tokens = add_tokens + added_tags\n",
    "    return add_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_enumerate = False\n",
    "vocab_txt = \"../input/transformers_cache/bert_large_uncased_vocab.txt\"\n",
    "tokenizer = tokenizer_class(vocab_txt, do_lower_case=do_lower_case)\n",
    "tags = get_add_tokens(do_enumerate=do_enumerate)\n",
    "num_added = tokenizer.add_tokens(tags)\n",
    "# print(\"Added {} tokens\".format(num_added))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"../input/nq_bert_uncased_68\"\n",
    "weights_fn = os.path.join(checkpoint_dir, 'weights.h5')\n",
    "model = model_class(config)\n",
    "model(model.dummy_inputs, training=False)\n",
    "model.load_weights(weights_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NQExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NQExample = collections.namedtuple(\"NQExample\", [\n",
    "    \"qas_id\", \"question_text\", \"doc_tokens\",\"crop_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting document text into crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Crop = collections.namedtuple(\"Crop\", [\"example_id\",\"unique_id\", \"doc_span_index\",\n",
    "    \"tokens\", \"token_to_orig_map\", \"token_is_max_context\",\n",
    "    \"input_ids\", \"attention_mask\", \"token_type_ids\",\n",
    "    \"paragraph_len\"])\n",
    "\n",
    "DocSpan = collections.namedtuple(\"DocSpan\", [\"start\", \"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(doc_stride, max_tokens_for_doc, max_len):\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < max_len:\n",
    "        length = max_len - start_offset\n",
    "        if length > max_tokens_for_doc:\n",
    "            length = max_tokens_for_doc\n",
    "        doc_spans.append(DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == max_len:\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "    return doc_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_crops(example, tokenizer,UNMAPPED=-123, max_seq_length=512,\n",
    "                              doc_stride=256, max_query_length=64, is_training=False,\n",
    "                              cls_token='[CLS]', sep_token='[SEP]', pad_id=0,\n",
    "                              sequence_a_segment_id=0,\n",
    "                              sequence_b_segment_id=1,\n",
    "                              cls_token_segment_id=0,\n",
    "                              pad_token_segment_id=0,\n",
    "                              mask_padding_with_zero=True,\n",
    "                              sep_token_extra=False):\n",
    "    \"\"\"Loads an example into a list of `InputBatch`s.\"\"\"\n",
    "    unique_id = 1000000000\n",
    "    sub_token_cache = {}\n",
    "    \n",
    "    crops = []\n",
    "\n",
    "    query_tokens = tokenizer.tokenize(example.question_text)\n",
    "    if len(query_tokens) > max_query_length:\n",
    "        query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "    # this takes the longest!\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "\n",
    "    for i, token in enumerate(example.doc_tokens):\n",
    "        orig_to_tok_index.append(len(all_doc_tokens))\n",
    "        sub_tokens = sub_token_cache.get(token)\n",
    "        if sub_tokens is None:\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            sub_token_cache[token] = sub_tokens\n",
    "        tok_to_orig_index.extend([i for _ in range(len(sub_tokens))])\n",
    "        all_doc_tokens.extend(sub_tokens)\n",
    "\n",
    "    tok_start_position = None\n",
    "    tok_end_position = None\n",
    "\n",
    "    # For Bert: [CLS] question [SEP] paragraph [SEP]\n",
    "    special_tokens_count = 3\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - special_tokens_count\n",
    "    assert max_tokens_for_doc > 0\n",
    "    # We can have documents that are longer than the maximum\n",
    "    # sequence length. To deal with this we do a sliding window\n",
    "    # approach, where we take chunks of the up to our max length\n",
    "    # with a stride of `doc_stride`.\n",
    "    doc_spans = get_spans(doc_stride, max_tokens_for_doc, len(all_doc_tokens))\n",
    "    for doc_span_index, doc_span in enumerate(doc_spans):\n",
    "        # Tokens are constructed as: CLS Query SEP Paragraph SEP\n",
    "        tokens = []\n",
    "        token_to_orig_map = UNMAPPED * np.ones((max_seq_length, ), dtype=np.int32)\n",
    "        token_is_max_context = np.zeros((max_seq_length, ), dtype=np.bool)\n",
    "        token_type_ids = []\n",
    "        special_tokens_offset = special_tokens_count - 1\n",
    "        doc_offset = len(query_tokens) + special_tokens_offset\n",
    "\n",
    "        # CLS token at the beginning\n",
    "        tokens.append(cls_token)\n",
    "        token_type_ids.append(cls_token_segment_id)\n",
    "\n",
    "        # Query\n",
    "        tokens += query_tokens\n",
    "        token_type_ids += [sequence_a_segment_id] * len(query_tokens)\n",
    "\n",
    "        # SEP token\n",
    "        tokens.append(sep_token)\n",
    "        token_type_ids.append(sequence_a_segment_id)\n",
    "\n",
    "        # Paragraph\n",
    "        for i in range(doc_span.length):\n",
    "            split_token_index = doc_span.start + i\n",
    "            # We add `example.crop_start` as the original document\n",
    "            # is already shifted\n",
    "            token_to_orig_map[len(tokens)] = tok_to_orig_index[\n",
    "                split_token_index] + example.crop_start\n",
    "\n",
    "            token_is_max_context[len(tokens)] = check_is_max_context(doc_spans,\n",
    "                doc_span_index, split_token_index)\n",
    "            tokens.append(all_doc_tokens[split_token_index])\n",
    "            token_type_ids.append(sequence_b_segment_id)\n",
    "\n",
    "        paragraph_len = doc_span.length\n",
    "\n",
    "        # SEP token\n",
    "        tokens.append(sep_token)\n",
    "        token_type_ids.append(sequence_b_segment_id)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_id)\n",
    "            attention_mask.append(0 if mask_padding_with_zero else 1)\n",
    "            token_type_ids.append(pad_token_segment_id)\n",
    "\n",
    "        # reduce memory, only input_ids needs more bits\n",
    "        input_ids = np.array(input_ids, dtype=np.int32)\n",
    "        attention_mask = np.array(attention_mask, dtype=np.bool)\n",
    "        token_type_ids = np.array(token_type_ids, dtype=np.uint8)\n",
    "\n",
    "        crop = Crop(\n",
    "            example_id = example.qas_id,\n",
    "            unique_id=unique_id,\n",
    "            doc_span_index=doc_span_index,\n",
    "            tokens=tokens,\n",
    "            token_to_orig_map=token_to_orig_map,\n",
    "            token_is_max_context=token_is_max_context,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            paragraph_len=paragraph_len)\n",
    "        crops.append(crop)\n",
    "        unique_id += 1\n",
    "\n",
    "    return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 1\n",
    "RawResult = collections.namedtuple(\"RawResult\", [\"unique_id\", \"start_logits\", \"end_logits\",\"long_logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def predict_step(batch):\n",
    "    outputs = model(batch, training=False)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PrelimPrediction = collections.namedtuple(\"PrelimPrediction\",\n",
    "    [\"crop_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "NbestPrediction = collections.namedtuple(\"NbestPrediction\", [\n",
    "    \"text\", \"start_logit\", \"end_logit\",\n",
    "    \"start_index\", \"end_index\",\n",
    "    \"orig_doc_start\", \"orig_doc_end\", \"crop_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prelim_predict(example_id,n_best_size = 10,max_answer_length = 30,UNMAPPED = -123):\n",
    "    short_prelim_predictions = []\n",
    "    for crop_index, crop in enumerate(part_of_crops):\n",
    "        result = unique_id_to_result[crop.unique_id]\n",
    "        start_indexes = np.argpartition(result.start_logits, -n_best_size)[-n_best_size:]\n",
    "        start_indexes = [int(x) for x in start_indexes]\n",
    "        end_indexes = np.argpartition(result.end_logits, -n_best_size)[-n_best_size:]\n",
    "        end_indexes = [int(x) for x in end_indexes]\n",
    "\n",
    "        # create short answers\n",
    "        for start_index in start_indexes:\n",
    "            if start_index >= len(crop.tokens):\n",
    "                continue\n",
    "            if crop.token_to_orig_map[start_index] == UNMAPPED:\n",
    "                continue\n",
    "            if not crop.token_is_max_context[start_index]:\n",
    "                continue\n",
    "\n",
    "            for end_index in end_indexes:\n",
    "                if end_index >= len(crop.tokens):\n",
    "                    continue\n",
    "                if crop.token_to_orig_map[end_index] == UNMAPPED:\n",
    "                    continue\n",
    "                if end_index < start_index:\n",
    "                    continue\n",
    "                length = end_index - start_index + 1\n",
    "                if length > max_answer_length:\n",
    "                    continue\n",
    "\n",
    "                short_prelim_predictions.append(PrelimPrediction(\n",
    "                    crop_index=crop_index,\n",
    "                    start_index=start_index,\n",
    "                    end_index=end_index,\n",
    "                    start_logit=result.start_logits[start_index],\n",
    "                    end_logit=result.end_logits[end_index]))\n",
    "\n",
    "        short_prelim_predictions = sorted(short_prelim_predictions,\n",
    "            key=lambda x: x.start_logit + x.end_logit, reverse=True)\n",
    "    return short_prelim_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbest(prelim_predictions, crops, example, n_best_size=10):\n",
    "    seen, nbest = set(), []\n",
    "    for pred in prelim_predictions:\n",
    "        if len(nbest) >= n_best_size:\n",
    "            break\n",
    "        crop = crops[pred.crop_index]\n",
    "        orig_doc_start, orig_doc_end = -1, -1\n",
    "        # non-null\n",
    "        if pred.start_index > 0:\n",
    "            # Long answer has no end_index. We still generate some text to check\n",
    "            if pred.end_index == -1:\n",
    "                tok_tokens = crop.tokens[pred.start_index: pred.start_index + 11]\n",
    "            else:\n",
    "                tok_tokens = crop.tokens[pred.start_index: pred.end_index + 1]\n",
    "            tok_text = \" \".join(tok_tokens)\n",
    "            tok_text = clean_text(tok_text)\n",
    "\n",
    "            orig_doc_start = int(crop.token_to_orig_map[pred.start_index])\n",
    "            if pred.end_index == -1:\n",
    "                orig_doc_end = orig_doc_start + 10\n",
    "            else:\n",
    "                orig_doc_end = int(crop.token_to_orig_map[pred.end_index])\n",
    "\n",
    "            final_text = tok_text\n",
    "            if final_text in seen:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            final_text = \"\"\n",
    "\n",
    "        seen.add(final_text)\n",
    "        nbest.append(NbestPrediction(\n",
    "            text=final_text,\n",
    "            start_logit=pred.start_logit, end_logit=pred.end_logit,\n",
    "            start_index=pred.start_index, end_index=pred.end_index,\n",
    "            orig_doc_start=orig_doc_start, orig_doc_end=orig_doc_end,\n",
    "            crop_index=pred.crop_index))\n",
    "\n",
    "    # Degenerate case. I never saw this happen.\n",
    "    if len(nbest) in (0, 1):\n",
    "        nbest.insert(0, NbestPrediction(text=\"empty\",\n",
    "            start_logit=0.0, end_logit=0.0,\n",
    "            start_index=-1, end_index=-1,\n",
    "            orig_doc_start=-1, orig_doc_end=-1,\n",
    "            crop_index=UNMAPPED))\n",
    "\n",
    "    assert len(nbest) >= 1\n",
    "    return nbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tok_text):\n",
    "    # De-tokenize WordPieces that have been split off.\n",
    "    tok_text = tok_text.replace(\" ##\", \"\")\n",
    "    tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "    # Clean whitespace\n",
    "    tok_text = tok_text.strip()\n",
    "    tok_text = \" \".join(tok_text.split())\n",
    "    return tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tokens(paragraph_text):\n",
    "    doc_tokens,char_to_word_offset = [],[]\n",
    "    prev_is_whitespace = True\n",
    "    for c in paragraph_text:\n",
    "        if is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                doc_tokens.append(c)\n",
    "            else:\n",
    "                doc_tokens[-1] += c\n",
    "            prev_is_whitespace = False\n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['Cristiano Ronaldo personal information']\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Cristiano_Ronaldo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "\n",
    "html = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# paragraphs = html.select(\"p\")\n",
    "# document_text = '\\n'.join([ para.text for para in paragraphs])\n",
    "\n",
    "paragraphs = html.select(\"table\")\n",
    "document_text = '\\n'.join([ str(table) for table in paragraphs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 77 paragraphs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-dcf80972942c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mexample_indexes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'example_index'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mbatched_start_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mbatched_end_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    492\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"There are {} paragraphs\".format(len(paragraphs)))\n",
    "doc_tokens = get_doc_tokens(document_text)\n",
    "for example_id,question in enumerate(questions) :\n",
    "    tic = time.time()\n",
    "    \n",
    "    ## build NQExample\n",
    "    qa = {'question': question, 'id': example_id, 'crop_start': 0}\n",
    "    example = NQExample(\n",
    "            qas_id=qa[\"id\"],\n",
    "            question_text=qa[\"question\"],\n",
    "            doc_tokens=doc_tokens,\n",
    "            crop_start=qa[\"crop_start\"])\n",
    "    \n",
    "    ## compute crops\n",
    "    crops = convert_examples_to_crops(example, tokenizer)\n",
    "    \n",
    "    ## build dataset\n",
    "    all_input_ids = tf.stack([c.input_ids for c in crops], 0)\n",
    "    all_attention_mask = tf.stack([c.attention_mask for c in crops], 0)\n",
    "    all_token_type_ids = tf.stack([c.token_type_ids for c in crops], 0)\n",
    "    dataset = [all_input_ids, all_attention_mask, all_token_type_ids]\n",
    "    eval_ds = tf.data.Dataset.from_tensor_slices({\n",
    "    'input_ids': tf.constant(dataset[0]),\n",
    "    'attention_mask': tf.constant(dataset[1]),\n",
    "    'token_type_ids': tf.constant(dataset[2]),\n",
    "    'example_index': tf.range(len(dataset[0]), dtype=tf.int32)\n",
    "    })\n",
    "    eval_ds = eval_ds.batch(batch_size=eval_batch_size, drop_remainder=True)\n",
    "    \n",
    "    ## raw results making\n",
    "    all_results = []\n",
    "    for batch in eval_ds:\n",
    "        example_indexes = batch['example_index']\n",
    "        outputs = predict_step(batch)\n",
    "        batched_start_logits = outputs[0].numpy()\n",
    "        batched_end_logits = outputs[1].numpy()\n",
    "        batched_long_logits = outputs[2].numpy()\n",
    "        for i, example_index in enumerate(example_indexes):\n",
    "\n",
    "            eval_crop = crops[example_index]\n",
    "            unique_id = int(eval_crop.unique_id)\n",
    "            start_logits = batched_start_logits[i].tolist()\n",
    "            end_logits = batched_end_logits[i].tolist()\n",
    "            long_logits = batched_long_logits[i].tolist()\n",
    "\n",
    "            result = RawResult(unique_id=unique_id,\n",
    "                               start_logits=start_logits,\n",
    "                               end_logits=end_logits,\n",
    "                               long_logits=long_logits)\n",
    "            all_results.append(result)\n",
    "    \n",
    "    ## premilinary predictions\n",
    "    example_index_to_crops = collections.defaultdict(list)\n",
    "    for crop in crops :\n",
    "        example_index_to_crops[crop.example_id].append(crop)\n",
    "    unique_id_to_result = {result.unique_id: result for result in all_results}\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    part_of_crops = example_index_to_crops[example_id]\n",
    "    short_prelim_predictions = prelim_predict(example_id)\n",
    "    short_nbest = get_nbest(short_prelim_predictions, part_of_crops,example)\n",
    "    \n",
    "    ## Show results\n",
    "    short_best_non_null = short_nbest[0].text\n",
    "    for entry in short_nbest[1:]:\n",
    "        if len(entry.text) > len(short_best_non_null) and short_best_non_null in entry.text:\n",
    "                short_best_non_null = \" \".join(doc_tokens[entry.orig_doc_start:entry.orig_doc_end])\n",
    "    print(\"Quesion :\",question)\n",
    "    print(\"Answer :\",short_best_non_null)\n",
    "    print(\"Finding answer time :\",round(time.time() - tic,1),\"s\")\n",
    "    print(\"-----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
